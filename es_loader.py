from elasticsearch import Elasticsearch, ConnectionError
from sentence_transformers import SentenceTransformer  # å‘é‡åŒ–æ¨¡å‹
from dotenv import load_dotenv
import os
import json
import uuid
import re

load_dotenv()

ES_HOST = os.getenv("ELASTICSEARCH_HOST", "http://localhost:9200")

""" é€£æ¥åˆ° ES """
try:
    es = Elasticsearch(ES_HOST)

    # æ¸¬è©¦é€£ç·š
    if not es.ping():
        raise ConnectionError("âŒ ç„¡æ³•é€£æ¥åˆ° Elasticsearchï¼Œè«‹æª¢æŸ¥ä¼ºæœå™¨æ˜¯å¦æ­£åœ¨é‹è¡Œã€‚")

    print("âœ… æˆåŠŸé€£æ¥åˆ° Elasticsearch")
except ConnectionError as e:
    print(f"ğŸ”´ Elasticsearch é€£ç·šéŒ¯èª¤: {e}")
    exit(1)  # é€€å‡ºç¨‹å¼

""" åˆå§‹åŒ– embedding model """
model = SentenceTransformer("sentence-transformers/distiluse-base-multilingual-cased-v2")


# """ ç´¢å¼•å„ªåŒ–ï¼šæŠ½å–åƒç…§æ³•æº """
def extract_references(content, current_law):
    """
    å›å‚³ä¸€å€‹ list of dictï¼Œæ¯å€‹ dict æ ¼å¼ç‚ºï¼š
    {
        "law_name": "åœ‹ç±æ³•",
        "article": "ç¬¬3æ¢",
        "paragraph": "ç¬¬1é …",
        "subparagraph": "ç¬¬1æ¬¾"
    }
    """
    if not content:
        return []

    results = []

    # é¡¯æ€§å¼•ç”¨
    explicit_block_pattern = re.compile(
        r"(?:ä¾|æ ¹æ“š|æŒ‰|åƒç…§)[\sã€Œã€]*"
        r"(æœ¬æ³•|[\u4e00-\u9fa5]{2,6}æ³•)"
        r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
        r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*))?"
        r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*))?"
    )

    for match in explicit_block_pattern.finditer(content):
        law_name, article, item_block, subitem_block = match.groups()
        full_law = current_law if law_name == "æœ¬æ³•" else law_name

        paragraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …", item_block or "") or [None]
        subparagraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾", subitem_block or "") or [None]

        for p in paragraphs:
            for s in subparagraphs:
                results.append({
                    "law_name": full_law,
                    "article": article,
                    "paragraph": p,
                    "subparagraph": s
                })

    # éš±æ€§å¼•ç”¨
    implicit_pattern = re.compile(
        r"(?<!æ³•)(?<![\u4e00-\u9fa5])"
        r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
        r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*))?"
        r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*))?"
    )

    for match in implicit_pattern.finditer(content):
        article, item_block, subitem_block = match.groups()

        paragraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …", item_block or "") or [None]
        subparagraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾", subitem_block or "") or [None]

        for p in paragraphs:
            for s in subparagraphs:
                results.append({
                    "law_name": current_law,
                    "article": article,
                    "paragraph": p,
                    "subparagraph": s
                })

    return results

# def extract_references(content, current_law):
#     """
#     å¾æ¢æ–‡å…§å®¹ä¸­æŠ½å–åƒç…§æ³•æºï¼Œæ”¯æ´ï¼š
#     1. é¡¯æ€§å¼•ç”¨ï¼šå¦‚ã€Œä¾æœ¬æ³•ç¬¬åä¸€æ¢ç¬¬äºŒé …ã€ç¬¬ä¸‰é …ã€
#     2. éš±æ€§å¼•ç”¨ï¼šå¦‚ã€Œä¾ç¬¬äºŒåäº”æ¢ç¬¬ä¸‰é …ã€ï¼Œé è¨­ç‚º current_law
#     3. æ¢æ–‡æ“´å±•ï¼šç¬¬åä¸€æ¢ç¬¬äºŒé …ã€ç¬¬ä¸‰é … â†’ ç¬¬11æ¢ç¬¬2é …ã€ç¬¬11æ¢ç¬¬3é …
#     """
#     if not content:
#         return []

#     references = set()

#     # é¡¯æ€§å¼•ç”¨
#     explicit_block_pattern = re.compile(
#         r"(?:ä¾|æ ¹æ“š|æŒ‰|åƒç…§)[\sã€Œã€]*"  # å‰ç¶´è©
#         r"(æœ¬æ³•|[\u4e00-\u9fa5]{2,6}æ³•)"  # æ³•å
#         r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"  # æ¢
#         r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*))?"  # é …
#         r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*))?"  # æ¬¾
#     )

#     for match in explicit_block_pattern.finditer(content):
#         law_name, article, item_block, subitem_block = match.groups()
#         full_law = current_law if law_name == "æœ¬æ³•" else law_name
#         references.add(f"{full_law}-{article}")

#         if item_block:
#             for item in re.findall(r"ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+)é …", item_block):
#                 references.add(f"{full_law}-{article}ç¬¬{item}é …")

#         if subitem_block:
#             for subitem in re.findall(r"ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+)æ¬¾", subitem_block):
#                 references.add(f"{full_law}-{article}ç¬¬{subitem}æ¬¾")

#     # éš±æ€§å¼•ç”¨ï¼ˆå¦‚ï¼šç¬¬äºŒåäº”æ¢ç¬¬ä¸‰é …ï¼‰
#     implicit_pattern = re.compile(
#         r"(?<!æ³•)(?<![\u4e00-\u9fa5])" 
#         r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
#         r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*))?"
#         r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*))?"
#     )

#     for match in implicit_pattern.finditer(content):
#         article, item_block, subitem_block = match.groups()
#         references.add(f"{current_law}-{article}")

#         if item_block:
#             for item in re.findall(r"ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+)é …", item_block):
#                 references.add(f"{current_law}-{article}ç¬¬{item}é …")

#         if subitem_block:
#             for subitem in re.findall(r"ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+)æ¬¾", subitem_block):
#                 references.add(f"{current_law}-{article}ç¬¬{subitem}æ¬¾")

#     return list(references)

""" è™•ç† immigration-law.json æ ¼å¼ """
def handle_law(json_data, index_name):
    # åˆ¤æ–·æ˜¯å¦ç‚ºç« ç¯€åˆ¶
    is_chapter_based = "ç« ç¯€" in json_data

    # æ ¹æ“šçµæ§‹å–å¾—æ¢æ–‡æ¸…å–®
    if is_chapter_based:
        documents = json_data.get("ç« ç¯€", [])
    else:
        documents = [{"ç« å": None, "æ¢æ–‡": json_data.get("æ¢æ–‡", [])}]

    if not documents or all(not chapter.get("æ¢æ–‡") for chapter in documents):
        print(f"âš ï¸ æª”æ¡ˆç¼ºå°‘æ¢æ–‡è³‡æ–™")
        return 0, 0

    # å»ºç«‹ indexï¼ˆè‹¥å°šæœªå­˜åœ¨ï¼‰
    if not es.indices.exists(index=index_name):
        es.indices.create(index=index_name, body={
            "mappings": {
                "properties": {
                    "name": {"type": "keyword"},
                    "date": {"type": "keyword"},
                    "chapter_title": {"type": "keyword"},
                    "number": {"type": "keyword"},
                    "reference_laws": {
                        "type": "nested",
                        "properties": {
                            "law_name": {"type": "keyword"},
                            "article": {"type": "keyword"},
                            "paragraph": {"type": "keyword"},
                            "subparagraph": {"type": "keyword"}
                        }
                    },
                    "content": {"type": "text"},
                    "embedding": {"type": "dense_vector", "dims": 512}
                }
            }
        })
        print(f"âœ… å»ºç«‹ indexï¼š{index_name}")
    else:
        print(f"â„¹ï¸ ç´¢å¼•å·²å­˜åœ¨ï¼š{index_name}")

    success, failed = 0, 0
    for chapter in documents:
        chapter_title = chapter.get("ç« å", "") or ""
        for clause in chapter.get("æ¢æ–‡", []):
            clause_id = clause.get("æ¢è™Ÿ")
            content = clause.get("å…§å®¹", "")

            if not clause_id or not content:
                failed += 1
                continue

            embedding = model.encode(content).tolist()
            doc_id = f"{chapter_title}_{clause_id}".replace(" ", "")

            # åƒç…§æ³•æº
            reference_laws = extract_references(content, json_data.get("æ³•è¦åç¨±", ""))

            # è‹¥å·²å­˜åœ¨å°±è·³éè™•ç†
            if es.exists(index=index_name, id=doc_id):
                continue

            try:
                es.index(index=index_name, id=doc_id, body={
                    "name": json_data.get("æ³•è¦åç¨±", ""),
                    "date": json_data.get("ä¿®æ­£æ—¥æœŸ", ""),
                    "chapter_title": chapter_title,
                    "number": clause_id,
                    "reference_laws": reference_laws, 
                    "content": content,
                    "embedding": embedding
                })
                success += 1
            except Exception as e:
                print(f"âŒ å¯«å…¥å¤±æ•—ï¼š{clause_id} - {e}")
                failed += 1

    return success, failed


""" åœ¨é€™è£¡å¢åŠ å…¶ä»–è™•ç†æ ¼å¼ """


""" è³‡æ–™ä¾†æºè·¯å¾‘ """
DATA_DIR = "data"
files = [f for f in os.listdir(DATA_DIR) if f.endswith(".json")]

""" æ ¹æ“šæª”åé¸æ“‡è™•ç†é‚è¼¯ """
for filename in files:
    path = os.path.join(DATA_DIR, filename)
    try:
        with open(path, "r", encoding="utf-8") as f:
            json_data = json.load(f)
    except Exception as e:
        print(f"ğŸ”´ ç„¡æ³•è®€å– {filename}ï¼š{e}")
        continue

    # è‡ªå‹•ç”¨æª”åè½‰ index å
    base = os.path.splitext(filename)[0]  # e.g. "labor-law"
    index_name = f"ai_{base}_index"

    print(f"\nğŸ“„ è™•ç†æª”æ¡ˆï¼š{filename} â†’ Index: {index_name}")

    # æ ¹æ“šæª”æ¡ˆåç¨±æ±ºå®šæ ¼å¼è™•ç†æ–¹å¼
    # if "labor-law" in base:
    #     success, failed = handle_labor_law(json_data, index_name)
    if any(keyword in base for keyword in ["immigration-law", "immigration-regulations", "nationality-law"]):
        success, failed = handle_law(json_data, index_name)
    else:
        print(f"âš ï¸ å°šæœªæ”¯æ´æ­¤æª”æ¡ˆæ ¼å¼ï¼š{filename}")
        continue

    print(f"âœ… å¯«å…¥æˆåŠŸï¼š{success} ç­†ï¼Œâš ï¸ å¤±æ•—ï¼š{failed} ç­†")

print("\nğŸ“Œ æ‰€æœ‰æª”æ¡ˆè™•ç†å®Œæˆï¼")





from elasticsearch import Elasticsearch, ConnectionError
from sentence_transformers import SentenceTransformer  # å‘é‡åŒ–æ¨¡å‹
from dotenv import load_dotenv
import os
import json
import uuid

load_dotenv()

ES_HOST = os.getenv("ELASTICSEARCH_HOST", "http://localhost:9200")

""" é€£æ¥åˆ° ES """
try:
    es = Elasticsearch(ES_HOST)

    # æ¸¬è©¦é€£ç·š
    if not es.ping():
        raise ConnectionError("âŒ ç„¡æ³•é€£æ¥åˆ° Elasticsearchï¼Œè«‹æª¢æŸ¥ä¼ºæœå™¨æ˜¯å¦æ­£åœ¨é‹è¡Œã€‚")

    print("âœ… æˆåŠŸé€£æ¥åˆ° Elasticsearch")
except ConnectionError as e:
    print(f"ğŸ”´ Elasticsearch é€£ç·šéŒ¯èª¤: {e}")
    exit(1)  # é€€å‡ºç¨‹å¼

""" åˆå§‹åŒ– embedding model """
model = SentenceTransformer("sentence-transformers/distiluse-base-multilingual-cased-v2")

""" è™•ç† labor-law.json æ ¼å¼ """
# def handle_labor_law(json_data, index_name):
#     documents = json_data.get("ç« ç¯€", [])
#     if not documents:
#         print(f"âš ï¸ æª”æ¡ˆç¼ºå°‘ 'ç« ç¯€'")
#         return 0, 0

#     if not es.indices.exists(index=index_name):
#         es.indices.create(index=index_name, body={
#             "mappings": {
#                 "properties": {
#                     "name": {"type": "keyword"},
#                     "date": {"type": "keyword"},
#                     "chapter_title": {"type": "keyword"},
#                     "number": {"type": "keyword"},
#                     "content": {"type": "text"},
#                     "embedding": {"type": "dense_vector", "dims": 512}
#                 }
#             }
#         })
#         print(f"âœ… å»ºç«‹ indexï¼š{index_name}")
#     else:
#         print(f"â„¹ï¸ ç´¢å¼•å·²å­˜åœ¨ï¼š{index_name}")

#     success, failed = 0, 0
#     for chapter in documents:
#         for clause in chapter.get("æ¢æ¬¾", []):
#             clause_id = clause.get("æ¢è™Ÿ")
#             content = clause.get("å…§å®¹", "")
            
#             if not clause_id or not content:
#                 failed += 1
#                 continue

#             embedding = model.encode(content).tolist()
#             doc_id = f"{chapter.get('ç« å', '')}_{clause_id}".replace(" ", "")

#             # è‹¥å·²å­˜åœ¨å°±è·³éè™•ç†
#             if es.exists(index=index_name, id=doc_id):
#                 continue

#             try:
#                 es.index(index=index_name, id=doc_id, body={
#                     "name": json_data.get("æ³•è¦åç¨±", ""),
#                     "date": json_data.get("ä¿®æ­£æ—¥æœŸ", ""),
#                     "chapter_title": chapter.get("ç« ç¯€æ¨™é¡Œ", ""),
#                     "number": clause_id,
#                     "content": content,
#                     "embedding": embedding
#                 })
#                 success += 1
#             except Exception as e:
#                 print(f"âŒ å¯«å…¥å¤±æ•—ï¼š{clause_id} - {e}")
#                 failed += 1

#     return success, failed

""" è™•ç† immigration-law.json æ ¼å¼ """
def handle_immigration_law(json_data, index_name):
    documents = json_data.get("ç« ç¯€", [])
    if not documents:
        print(f"âš ï¸ æª”æ¡ˆç¼ºå°‘ 'ç« ç¯€'")
        return 0, 0

    if not es.indices.exists(index=index_name):
        es.indices.create(index=index_name, body={
            "mappings": {
                "properties": {
                    "name": {"type": "keyword"},
                    "date": {"type": "keyword"},
                    "chapter_title": {"type": "keyword"},
                    "number": {"type": "keyword"},
                    "content": {"type": "text"},
                    "embedding": {"type": "dense_vector", "dims": 512}
                }
            }
        })
        print(f"âœ… å»ºç«‹ indexï¼š{index_name}")
    else:
        print(f"â„¹ï¸ ç´¢å¼•å·²å­˜åœ¨ï¼š{index_name}")

    success, failed = 0, 0
    for chapter in documents:
        for clause in chapter.get("æ¢æ–‡", []):
            clause_id = clause.get("æ¢è™Ÿ")
            content = clause.get("å…§å®¹", "")
            
            if not clause_id or not content:
                failed += 1
                continue

            embedding = model.encode(content).tolist()
            doc_id = f"{chapter.get('ç« å', '')}_{clause_id}".replace(" ", "")

            # è‹¥å·²å­˜åœ¨å°±è·³éè™•ç†
            if es.exists(index=index_name, id=doc_id):
                continue

            try:
                es.index(index=index_name, id=doc_id, body={
                    "name": json_data.get("æ³•è¦åç¨±", ""),
                    "date": json_data.get("ä¿®æ­£æ—¥æœŸ", ""),
                    "chapter_title": chapter.get("ç« å", ""),
                    "number": clause_id,
                    "content": content,
                    "embedding": embedding
                })
                success += 1
            except Exception as e:
                print(f"âŒ å¯«å…¥å¤±æ•—ï¼š{clause_id} - {e}")
                failed += 1

    return success, failed


""" åœ¨é€™è£¡å¢åŠ å…¶ä»–è™•ç†æ ¼å¼ """


""" è³‡æ–™ä¾†æºè·¯å¾‘ """
DATA_DIR = "data"
files = [f for f in os.listdir(DATA_DIR) if f.endswith(".json")]

""" æ ¹æ“šæª”åé¸æ“‡è™•ç†é‚è¼¯ """
for filename in files:
    path = os.path.join(DATA_DIR, filename)
    try:
        with open(path, "r", encoding="utf-8") as f:
            json_data = json.load(f)
    except Exception as e:
        print(f"ğŸ”´ ç„¡æ³•è®€å– {filename}ï¼š{e}")
        continue

    # è‡ªå‹•ç”¨æª”åè½‰ index å
    base = os.path.splitext(filename)[0]  # e.g. "labor-law"
    index_name = f"ai_{base}_index"

    print(f"\nğŸ“„ è™•ç†æª”æ¡ˆï¼š{filename} â†’ Index: {index_name}")

    # æ ¹æ“šæª”æ¡ˆåç¨±æ±ºå®šæ ¼å¼è™•ç†æ–¹å¼
    # if "labor-law" in base:
    #     success, failed = handle_labor_law(json_data, index_name)
    if "immigration-law" in base:
        success, failed = handle_immigration_law(json_data, index_name)
    else:
        print(f"âš ï¸ å°šæœªæ”¯æ´æ­¤æª”æ¡ˆæ ¼å¼ï¼š{filename}")
        continue

    print(f"âœ… å¯«å…¥æˆåŠŸï¼š{success} ç­†ï¼Œâš ï¸ å¤±æ•—ï¼š{failed} ç­†")

print("\nğŸ“Œ æ‰€æœ‰æª”æ¡ˆè™•ç†å®Œæˆï¼")





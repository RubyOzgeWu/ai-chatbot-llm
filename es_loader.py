from elasticsearch import Elasticsearch, ConnectionError, helpers
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
from datetime import datetime

import os
import json
import re

load_dotenv()

ES_HOST = os.getenv("ELASTICSEARCH_HOST")
ES_API_KEY = os.getenv("ELASTICSEARCH_API_KEY")

# ä¸»æ³•ï¼ç´°å‰‡å°æ‡‰è¡¨
LAW_RELATIONS = {
    "å…¥å‡ºåœ‹åŠç§»æ°‘æ³•æ–½è¡Œç´°å‰‡": "å…¥å‡ºåœ‹åŠç§»æ°‘æ³•",
    "åœ‹ç±æ³•æ–½è¡Œç´°å‰‡": "åœ‹ç±æ³•"
}

LAW_FAMILY = {
    "å…¥å‡ºåœ‹åŠç§»æ°‘æ³•": "å…¥å‡ºåœ‹åŠç§»æ°‘ç³»åˆ—",
    "å…¥å‡ºåœ‹åŠç§»æ°‘æ³•æ–½è¡Œç´°å‰‡": "å…¥å‡ºåœ‹åŠç§»æ°‘ç³»åˆ—",
    "åœ‹ç±æ³•": "åœ‹ç±æ³•ç³»åˆ—"
}

# é€£ç·šæª¢æŸ¥
try:
    es = Elasticsearch(ES_HOST, api_key=ES_API_KEY, verify_certs=True)
    if not es.ping():
        raise ConnectionError("âŒ ç„¡æ³•é€£æ¥åˆ° Elasticsearchã€‚")
    print("âœ… æˆåŠŸé€£æ¥åˆ° Elasticsearch")
except ConnectionError as e:
    print(f"ğŸ”´ é€£ç·šéŒ¯èª¤: {e}")
    exit(1)

# å‘é‡æ¨¡å‹
model = SentenceTransformer("sentence-transformers/distiluse-base-multilingual-cased-v2")

# ä¸­æ–‡æ•¸å­—è½‰æ•´æ•¸
CHINESE_NUMERAL_MAP = {
    "é›¶": 0, "ä¸€": 1, "äºŒ": 2, "ä¸‰": 3, "å››": 4,
    "äº”": 5, "å…­": 6, "ä¸ƒ": 7, "å…«": 8, "ä¹": 9,
    "å": 10, "ç™¾": 100, "åƒ": 1000
}

def chinese_numeral_to_int(text):
    result = 0
    tmp = 0
    units = {"å": 10, "ç™¾": 100, "åƒ": 1000}
    for ch in text:
        if ch in units:
            unit = units[ch]
            if tmp == 0:
                tmp = 1
            result += tmp * unit
            tmp = 0
        else:
            tmp = CHINESE_NUMERAL_MAP.get(ch, 0)
    result += tmp
    return result

def normalize_article(article_text):
    m = re.match(r"ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶\d]+)æ¢", article_text)
    if m:
        arabic = chinese_numeral_to_int(m.group(1))
        return f"ç¬¬{arabic}æ¢"
    return article_text

def extract_references(content, current_law):
    if not content:
        return []
    results = []
    alias_map = {
        "æœ¬æ³•": LAW_RELATIONS.get(current_law, current_law),
        "æœ¬ç´°å‰‡": current_law
    }

    # explicit patterns
    explicit_patterns = [
        re.compile(
            r"(?:ä¾|æ ¹æ“š|æŒ‰|åƒç…§)[\sã€Œã€]*"
            r"(æœ¬æ³•|æœ¬ç´°å‰‡|[\u4e00-\u9fa5]{2,6}æ³•)"
            r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
            r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*)?"
            r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*)?"
        ),
        re.compile(
            r"(æœ¬æ³•|æœ¬ç´°å‰‡|[\u4e00-\u9fa5]{2,}æ³•)"
            r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
            r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)?"
            r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)?"
            r"(æ‰€ç¨±|æ‰€å®š|æ‰€è¬‚)"
        )
    ]

    for pattern in explicit_patterns:
        for match in pattern.finditer(content):
            law_name, raw_article, item_block, subitem_block = match.groups()[:4]
            full_law = alias_map.get(law_name, law_name)
            article = normalize_article(raw_article)

            paragraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …", item_block or "") or [None]
            subparagraphs = re.findall(
                r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)",
                subitem_block or ""
            ) or [None]
            subparagraphs = [s.replace("æˆ–", "") if s else None for s in subparagraphs]

            ref_type = (
                "å…§éƒ¨å¼•ç”¨" if full_law == current_law else
                "ä¸»æ³•æ´å¼•" if LAW_RELATIONS.get(current_law) == full_law else
                "è·¨æ³•æº"
            )

            for p in paragraphs:
                for s in subparagraphs:
                    results.append({
                        "law_name": full_law,
                        "article": article,
                        "paragraph": p,
                        "subparagraph": s,
                        "type": ref_type
                    })

    # fallback
    fallback = re.compile(
        r"(?:ä¾|æ ¹æ“š|æŒ‰|åƒç…§)[\sã€Œã€]*"
        r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
        r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*)?"
        r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*)?"
    )
    for match in fallback.finditer(content):
        raw, item_block, subitem_block = match.groups()
        article = normalize_article(raw)
        paragraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …", item_block or "") or [None]
        subparagraphs = re.findall(
            r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)",
            subitem_block or ""
        ) or [None]
        subparagraphs = [s.replace("æˆ–", "") if s else None for s in subparagraphs]
        for p in paragraphs:
            for s in subparagraphs:
                results.append({
                    "law_name": current_law,
                    "article": article,
                    "paragraph": p,
                    "subparagraph": s,
                    "type": "å…§éƒ¨å¼•ç”¨"
                })

    # implicit
    implicit = re.compile(
        r"(?<!æ³•)(?<![\u4e00-\u9fa5])(?<!ä¾)(?<!æ ¹æ“š)(?<!æŒ‰)(?<!åƒç…§)"
        r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
        r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*)?"
        r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*)?"
    )
    for match in implicit.finditer(content):
        raw, item_block, subitem_block = match.groups()
        article = normalize_article(raw)
        paragraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …", item_block or "") or [None]
        subparagraphs = re.findall(
            r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)",
            subitem_block or ""
        ) or [None]
        subparagraphs = [s.replace("æˆ–", "") if s else None for s in subparagraphs]
        for p in paragraphs:
            for s in subparagraphs:
                results.append({
                    "law_name": current_law,
                    "article": article,
                    "paragraph": p,
                    "subparagraph": s,
                    "type": "å…§éƒ¨å¼•ç”¨"
                })

    return results

def handle_law(json_data, index_name):
    is_chapter_based = "ç« ç¯€" in json_data
    documents = json_data.get("ç« ç¯€", []) if is_chapter_based else [{"ç« å": None, "æ¢æ–‡": json_data.get("æ¢æ–‡", [])}]
    if not documents or all(not c.get("æ¢æ–‡") for c in documents):
        print("âš ï¸ æª”æ¡ˆç¼ºå°‘æ¢æ–‡è³‡æ–™")
        return 0, 0

    law_name = json_data.get("æ³•è¦åç¨±", "")
    law_type = "ç´°å‰‡" if law_name in LAW_RELATIONS else "ä¸»æ³•"
    law_family = LAW_FAMILY.get(law_name, "å…¶ä»–")

    success = failed = 0

    for chapter in documents:
        chap_title = chapter.get("ç« å") or ""
        for clause in chapter.get("æ¢æ–‡", []):
            num = clause.get("æ¢è™Ÿ")
            content = clause.get("å…§å®¹", "")
            if not num or not content:
                failed += 1
                continue

            try:
                emb = model.encode(content).tolist()
            except Exception as e:
                print(f"âŒ å‘é‡è½‰æ›å¤±æ•—ï¼š{num} - {e}")
                failed += 1
                continue

            doc_id = f"{chap_title}_{num}".replace(" ", "")
            refs = extract_references(content, law_name)

            parent = None
            if law_type == "ç´°å‰‡":
                links = [
                    {"article": r["article"], "paragraph": r["paragraph"], "subparagraph": r["subparagraph"]}
                    for r in refs if r["law_name"] == LAW_RELATIONS[law_name]
                ]
                if links:
                    parent = {"name": LAW_RELATIONS[law_name], "article_links": links}

            try:
                es.index(
                    index=index_name,
                    id=doc_id,
                    document={
                        "name": law_name,
                        "date": json_data.get("ä¿®æ­£æ—¥æœŸ", ""),
                        "chapter_title": chap_title,
                        "number": num,
                        "law_type": law_type,
                        "law_family": law_family,
                        "is_supplement": law_type == "ç´°å‰‡",
                        "parent_law": parent,
                        "reference_laws": refs,
                        "content": content,
                        "embedding": emb,
                        "@timestamp": datetime.utcnow().isoformat()
                    }
                )
                success += 1
            except Exception as e:
                print(f"âŒ å¯«å…¥å¤±æ•—ï¼š{num} - {type(e).__name__} - {e}")
                if hasattr(e, 'meta'):
                    print("ğŸ” meta status:", getattr(e.meta, 'status', 'N/A'))
                    print("ğŸ” meta url:", getattr(e.meta, 'url', 'N/A'))
                    print("ğŸ” meta headers:", getattr(e.meta, 'headers', 'N/A'))
                if hasattr(e, 'body'):
                    print("ğŸ” body:", e.body)
                failed += 1

    return success, failed

if __name__ == "__main__":
    DATA_DIR = "data"
    files = [f for f in os.listdir(DATA_DIR) if f.endswith(".json")]

    for fn in files:
        path = os.path.join(DATA_DIR, fn)
        try:
            with open(path, encoding="utf-8") as f:
                jd = json.load(f)
        except Exception as e:
            print(f"ğŸ”´ ç„¡æ³•è®€å– {fn}ï¼š{e}")
            continue

        base = os.path.splitext(fn)[0]               # e.g. "immigration-law"
        index_name = f"ai-laws-{base}"               # å°æ‡‰ä½ äº‹å…ˆåœ¨ DevTools å»ºçš„ ai-laws-* index
        print(f"\nğŸ“„ è™•ç†æª”æ¡ˆï¼š{fn} â†’ Index: {index_name}")

        if any(k in base for k in ["immigration-law", "immigration-regulations", "nationality-law"]):
            s, f = handle_law(jd, index_name)
            print(f"âœ… å¯«å…¥æˆåŠŸï¼š{s} ç­†ï¼Œâš ï¸ å¤±æ•—ï¼š{f} ç­†")
        else:
            print(f"âš ï¸ ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼š{fn}")

    print("\nğŸ“Œ æ‰€æœ‰æª”æ¡ˆè™•ç†å®Œæˆï¼")
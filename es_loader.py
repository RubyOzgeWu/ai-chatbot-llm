from elasticsearch import Elasticsearch, ConnectionError
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
import os
import json
import re

load_dotenv()

ES_HOST = os.getenv("ELASTICSEARCH_HOST", "http://localhost:9200")

# ä¸»æ³•ï¼ç´°å‰‡å°æ‡‰è¡¨
LAW_RELATIONS = {
    "å…¥å‡ºåœ‹åŠç§»æ°‘æ³•æ–½è¡Œç´°å‰‡": "å…¥å‡ºåœ‹åŠç§»æ°‘æ³•",
    "åœ‹ç±æ³•æ–½è¡Œç´°å‰‡": "åœ‹ç±æ³•"
}

LAW_FAMILY = {
    "å…¥å‡ºåœ‹åŠç§»æ°‘æ³•": "å…¥å‡ºåœ‹åŠç§»æ°‘ç³»åˆ—",
    "å…¥å‡ºåœ‹åŠç§»æ°‘æ³•æ–½è¡Œç´°å‰‡": "å…¥å‡ºåœ‹åŠç§»æ°‘ç³»åˆ—",
    "åœ‹ç±æ³•": "åœ‹ç±æ³•ç³»åˆ—"
}

try:
    es = Elasticsearch(ES_HOST)
    if not es.ping():
        raise ConnectionError("âŒ ç„¡æ³•é€£æ¥åˆ° Elasticsearchã€‚")
    print("âœ… æˆåŠŸé€£æ¥åˆ° Elasticsearch")
except ConnectionError as e:
    print(f"ğŸ”´ é€£ç·šéŒ¯èª¤: {e}")
    exit(1)

model = SentenceTransformer("sentence-transformers/distiluse-base-multilingual-cased-v2")

CHINESE_NUMERAL_MAP = {
    "é›¶": 0, "ä¸€": 1, "äºŒ": 2, "ä¸‰": 3, "å››": 4,
    "äº”": 5, "å…­": 6, "ä¸ƒ": 7, "å…«": 8, "ä¹": 9,
    "å": 10, "ç™¾": 100, "åƒ": 1000
}

def chinese_numeral_to_int(text):
    num_map = CHINESE_NUMERAL_MAP
    result = 0
    tmp = 0
    last_unit = 1

    units = {"å": 10, "ç™¾": 100, "åƒ": 1000}
    num = 0
    i = 0

    while i < len(text):
        ch = text[i]
        if ch in units:
            unit = units[ch]
            if tmp == 0:
                tmp = 1
            result += tmp * unit
            tmp = 0
            last_unit = unit
        else:
            tmp = num_map.get(ch, 0)
        i += 1

    result += tmp
    return result

def normalize_article(article_text):
    m = re.match(r"ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶\d]+)æ¢", article_text)
    if m:
        chinese_num = m.group(1)
        arabic = chinese_numeral_to_int(chinese_num)
        return f"ç¬¬{arabic}æ¢"
    return article_text

def extract_references(content, current_law):
    if not content:
        return []

    results = []

    alias_map = {
        "æœ¬æ³•": LAW_RELATIONS.get(current_law, current_law),
        "æœ¬ç´°å‰‡": current_law
    }

    explicit_patterns = [
        re.compile(
            r"(?:ä¾|æ ¹æ“š|æŒ‰|åƒç…§)[\sã€Œã€]*"
            r"(æœ¬æ³•|æœ¬ç´°å‰‡|[\u4e00-\u9fa5]{2,6}æ³•)"
            r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
            r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*)?"
            r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*)?"
        ),
        re.compile(
            r"(æœ¬æ³•|æœ¬ç´°å‰‡|[\u4e00-\u9fa5]{2,}æ³•)"
            r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
            r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)?"
            r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)?"
            r"(æ‰€ç¨±|æ‰€å®š|æ‰€è¬‚)"
        )
    ]

    for pattern in explicit_patterns:
        for match in pattern.finditer(content):
            law_name, raw_article, item_block, subitem_block = match.groups()[:4]
            full_law = alias_map.get(law_name, law_name)
            article = normalize_article(raw_article)

            paragraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …", item_block or "") or [None]
            subparagraphs = re.findall(r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)", subitem_block or "")
            subparagraphs = [s.replace("æˆ–", "") for s in subparagraphs] if subparagraphs else [None]

            ref_type = (
                "å…§éƒ¨å¼•ç”¨" if full_law == current_law else
                "ä¸»æ³•æ´å¼•" if current_law in LAW_RELATIONS and full_law == LAW_RELATIONS[current_law]
                else "è·¨æ³•æº"
            )

            for p in paragraphs:
                for s in subparagraphs:
                    results.append({
                        "law_name": full_law,
                        "article": article,
                        "paragraph": p,
                        "subparagraph": s,
                        "type": ref_type
                    })

    # fallbackï¼šä¾ç¬¬â—‹â—‹æ¢ï¼ˆæœªææ³•åï¼‰
    fallback_pattern = re.compile(
        r"(?:ä¾|æ ¹æ“š|æŒ‰|åƒç…§)[\sã€Œã€]*"
        r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
        r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*)?"
        r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*)?"
    )

    for match in fallback_pattern.finditer(content):
        raw_article, item_block, subitem_block = match.groups()
        article = normalize_article(raw_article)

        paragraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …", item_block or "") or [None]
        subparagraphs = re.findall(r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)", subitem_block or "")
        subparagraphs = [s.replace("æˆ–", "") for s in subparagraphs] if subparagraphs else [None]

        for p in paragraphs:
            for s in subparagraphs:
                results.append({
                    "law_name": current_law,
                    "article": article,
                    "paragraph": p,
                    "subparagraph": s,
                    "type": "å…§éƒ¨å¼•ç”¨"
                })

    # éš±æ€§æ¢è™Ÿï¼ˆç„¡å¼•è¨€è©ï¼‰
    implicit_pattern = re.compile(
        r"(?<!æ³•)(?<![\u4e00-\u9fa5])(?<!ä¾)(?<!æ ¹æ“š)(?<!æŒ‰)(?<!åƒç…§)"
        r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
        r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*)?"
        r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*)?"
    )

    for match in implicit_pattern.finditer(content):
        raw_article, item_block, subitem_block = match.groups()
        article = normalize_article(raw_article)

        paragraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …", item_block or "") or [None]
        subparagraphs = re.findall(r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)", subitem_block or "")
        subparagraphs = [s.replace("æˆ–", "") for s in subparagraphs] if subparagraphs else [None]

        for p in paragraphs:
            for s in subparagraphs:
                results.append({
                    "law_name": current_law,
                    "article": article,
                    "paragraph": p,
                    "subparagraph": s,
                    "type": "å…§éƒ¨å¼•ç”¨"
                })

    return results


# def extract_references(content, current_law):
#     if not content:
#         return []

#     results = []

#     alias_map = {
#         "æœ¬æ³•": LAW_RELATIONS.get(current_law, current_law),
#         "æœ¬ç´°å‰‡": current_law
#     }

#     explicit_patterns = [
#         re.compile(
#             r"(?:ä¾|æ ¹æ“š|æŒ‰|åƒç…§)[\sã€Œã€]*"
#             r"(æœ¬æ³•|æœ¬ç´°å‰‡|[\u4e00-\u9fa5]{2,6}æ³•)"
#             r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
#             r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*)?"
#             r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*)?"
#         ),
#         re.compile(
#             r"(æœ¬æ³•|æœ¬ç´°å‰‡|[\u4e00-\u9fa5]{2,}æ³•)"
#             r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
#             r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)?"
#             r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)?"
#             r"(æ‰€ç¨±|æ‰€å®š|æ‰€è¬‚)"
#         )
#     ]

#     for pattern in explicit_patterns:
#         for match in pattern.finditer(content):
#             law_name, article, item_block, subitem_block = match.groups()[:4]
#             full_law = alias_map.get(law_name, law_name)

#             paragraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …", item_block or "") or [None]
#             subparagraphs = re.findall(r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)", subitem_block or "")
#             subparagraphs = [s.replace("æˆ–", "") for s in subparagraphs] if subparagraphs else [None]

#             ref_type = (
#                 "å…§éƒ¨å¼•ç”¨" if full_law == current_law else
#                 "ä¸»æ³•æ´å¼•" if current_law in LAW_RELATIONS and full_law == LAW_RELATIONS[current_law]
#                 else "è·¨æ³•æº"
#             )

#             for p in paragraphs:
#                 for s in subparagraphs:
#                     results.append({
#                         "law_name": full_law,
#                         "article": article,
#                         "paragraph": p,
#                         "subparagraph": s,
#                         "type": ref_type
#                     })

#     # éš±æ€§å¼•ç”¨ï¼šé è¨­ç‚º current_lawï¼Œæ¨™è¨˜ç‚ºå…§éƒ¨å¼•ç”¨
#     implicit_pattern = re.compile(
#         r"(?<!æ³•)(?<![\u4e00-\u9fa5])(?<!ä¾)(?<!æ ¹æ“š)(?<!æŒ‰)(?<!åƒç…§)"
#         r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
#         r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*)?"
#         r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*)?"
#     )

#     for match in implicit_pattern.finditer(content):
#         article, item_block, subitem_block = match.groups()
#         paragraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …", item_block or "") or [None]
#         subparagraphs = re.findall(r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)", subitem_block or "")
#         subparagraphs = [s.replace("æˆ–", "") for s in subparagraphs] if subparagraphs else [None]

#         for p in paragraphs:
#             for s in subparagraphs:
#                 results.append({
#                     "law_name": current_law,
#                     "article": article,
#                     "paragraph": p,
#                     "subparagraph": s,
#                     "type": "å…§éƒ¨å¼•ç”¨"
#                 })

#     return results

def handle_law(json_data, index_name):
    is_chapter_based = "ç« ç¯€" in json_data
    documents = json_data.get("ç« ç¯€", []) if is_chapter_based else [{"ç« å": None, "æ¢æ–‡": json_data.get("æ¢æ–‡", [])}]

    if not documents or all(not chapter.get("æ¢æ–‡") for chapter in documents):
        print("âš ï¸ æª”æ¡ˆç¼ºå°‘æ¢æ–‡è³‡æ–™")
        return 0, 0

    if not es.indices.exists(index=index_name):
        es.indices.create(index=index_name, body={
            "mappings": {
                "properties": {
                    "name": {"type": "keyword"},
                    "date": {"type": "keyword"},
                    "chapter_title": {"type": "keyword"},
                    "number": {"type": "keyword"},
                    "law_type": {"type": "keyword"},
                    "law_family": {"type": "keyword"},
                    "is_supplement": {"type": "boolean"},
                    "parent_law": {
                        "properties": {
                            "name": {"type": "keyword"},
                            "article_links": {
                                "type": "nested",
                                "properties": {
                                    "article": {"type": "keyword"},
                                    "paragraph": {"type": "keyword"},
                                    "subparagraph": {"type": "keyword"}
                                }
                            }
                        }
                    },
                    "reference_laws": {
                        "type": "nested",
                        "properties": {
                            "law_name": {"type": "keyword"},
                            "article": {"type": "keyword"},
                            "paragraph": {"type": "keyword"},
                            "subparagraph": {"type": "keyword"},
                            "type": {"type": "keyword"}
                        }
                    },
                    "content": {"type": "text"},
                    "embedding": {"type": "dense_vector", "dims": 512}
                }
            }
        })
        print(f"âœ… å»ºç«‹ indexï¼š{index_name}")
    else:
        print(f"â„¹ï¸ ç´¢å¼•å·²å­˜åœ¨ï¼š{index_name}")

    success, failed = 0, 0
    for chapter in documents:
        chapter_title = chapter.get("ç« å", "") or ""
        for clause in chapter.get("æ¢æ–‡", []):
            clause_id = clause.get("æ¢è™Ÿ")
            content = clause.get("å…§å®¹", "")
            if not clause_id or not content:
                failed += 1
                continue

            embedding = model.encode(content).tolist()
            doc_id = f"{chapter_title}_{clause_id}".replace(" ", "")

            law_name = json_data.get("æ³•è¦åç¨±", "")
            reference_laws = extract_references(content, law_name)
            law_type = "ç´°å‰‡" if law_name in LAW_RELATIONS else "ä¸»æ³•"
            law_family = LAW_FAMILY.get(law_name, "å…¶ä»–")

            # parent_law only for ç´°å‰‡
            parent_law = None
            if law_type == "ç´°å‰‡":
                article_links = [
                    {
                        "article": ref["article"],
                        "paragraph": ref.get("paragraph"),
                        "subparagraph": ref.get("subparagraph")
                    }
                    for ref in reference_laws
                    if ref["law_name"] == LAW_RELATIONS[law_name]
                ]
                parent_law = {
                    "name": LAW_RELATIONS[law_name],
                    "article_links": article_links
                } if article_links else None

            if es.exists(index=index_name, id=doc_id):
                continue

            try:
                es.index(index=index_name, id=doc_id, body={
                    "name": law_name,
                    "date": json_data.get("ä¿®æ­£æ—¥æœŸ", ""),
                    "chapter_title": chapter_title,
                    "number": clause_id,
                    "law_type": law_type,
                    "law_family": law_family,
                    "is_supplement": law_type == "ç´°å‰‡",
                    "parent_law": parent_law,
                    "reference_laws": reference_laws,
                    "content": content,
                    "embedding": embedding
                })
                success += 1
            except Exception as e:
                print(f"âŒ å¯«å…¥å¤±æ•—ï¼š{clause_id} - {e}")
                failed += 1

    return success, failed

# è³‡æ–™ç›®éŒ„è™•ç†é‚è¼¯
DATA_DIR = "data"
files = [f for f in os.listdir(DATA_DIR) if f.endswith(".json")]

for filename in files:
    path = os.path.join(DATA_DIR, filename)
    try:
        with open(path, "r", encoding="utf-8") as f:
            json_data = json.load(f)
    except Exception as e:
        print(f"ğŸ”´ ç„¡æ³•è®€å– {filename}ï¼š{e}")
        continue

    base = os.path.splitext(filename)[0]
    index_name = f"ai_{base}_index"
    print(f"\nğŸ“„ è™•ç†æª”æ¡ˆï¼š{filename} â†’ Index: {index_name}")

    if any(k in base for k in ["immigration-law", "immigration-regulations", "nationality-law"]):
        success, failed = handle_law(json_data, index_name)
    else:
        print(f"âš ï¸ ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼š{filename}")
        continue

    print(f"âœ… å¯«å…¥æˆåŠŸï¼š{success} ç­†ï¼Œâš ï¸ å¤±æ•—ï¼š{failed} ç­†")

print("\nğŸ“Œ æ‰€æœ‰æª”æ¡ˆè™•ç†å®Œæˆï¼")








# from elasticsearch import Elasticsearch, ConnectionError
# from sentence_transformers import SentenceTransformer  # å‘é‡åŒ–æ¨¡å‹
# from dotenv import load_dotenv
# import os
# import json
# import uuid
# import re

# load_dotenv()

# ES_HOST = os.getenv("ELASTICSEARCH_HOST", "http://localhost:9200")

# """ é€£æ¥åˆ° ES """
# try:
#     es = Elasticsearch(ES_HOST)

#     # æ¸¬è©¦é€£ç·š
#     if not es.ping():
#         raise ConnectionError("âŒ ç„¡æ³•é€£æ¥åˆ° Elasticsearchï¼Œè«‹æª¢æŸ¥ä¼ºæœå™¨æ˜¯å¦æ­£åœ¨é‹è¡Œã€‚")

#     print("âœ… æˆåŠŸé€£æ¥åˆ° Elasticsearch")
# except ConnectionError as e:
#     print(f"ğŸ”´ Elasticsearch é€£ç·šéŒ¯èª¤: {e}")
#     exit(1)  # é€€å‡ºç¨‹å¼

# """ åˆå§‹åŒ– embedding model """
# model = SentenceTransformer("sentence-transformers/distiluse-base-multilingual-cased-v2")

# # ä¸»æ³•ï¼ç´°å‰‡å°æ‡‰è¡¨
# LAW_RELATIONS = {
#     "å…¥å‡ºåœ‹åŠç§»æ°‘æ³•æ–½è¡Œç´°å‰‡": "å…¥å‡ºåœ‹åŠç§»æ°‘æ³•",
#     "åœ‹ç±æ³•æ–½è¡Œç´°å‰‡": "åœ‹ç±æ³•"
# }

# def extract_references(content, current_law):
#     if not content:
#         return []

#     results = []

#     if current_law in LAW_RELATIONS:
#         alias_map = {
#             "æœ¬æ³•": LAW_RELATIONS[current_law],
#             "æœ¬ç´°å‰‡": current_law
#         }
#     else:
#         alias_map = {
#             "æœ¬æ³•": current_law,
#             "æœ¬ç´°å‰‡": current_law
#         }

#     explicit_block_pattern = re.compile(
#         r"(?:ä¾|æ ¹æ“š|æŒ‰|åƒç…§)[\sã€Œã€]*"
#         r"(æœ¬æ³•|æœ¬ç´°å‰‡|[\u4e00-\u9fa5]{2,6}æ³•)"
#         r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
#         r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*))?"
#         r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*)?"
#     )

#     extra_reference_pattern = re.compile(
#         r"(æœ¬æ³•|æœ¬ç´°å‰‡|[\u4e00-\u9fa5]{2,}æ³•)"
#         r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
#         r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)?"
#         r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)?"
#         r"(æ‰€ç¨±|æ‰€å®š|æ‰€è¬‚)"
#     )

#     for pattern in [explicit_block_pattern, extra_reference_pattern]:
#         for match in pattern.finditer(content):
#             law_name, article, item_block, subitem_block = match.groups()[:4]
#             full_law = alias_map.get(law_name, law_name)

#             paragraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …", item_block or "") or [None]
#             subparagraphs = re.findall(
#                 r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)",
#                 subitem_block or ""
#             )
#             subparagraphs = [s.replace("æˆ–", "") for s in subparagraphs] if subparagraphs else [None]

#             for p in paragraphs:
#                 for s in subparagraphs:
#                     ref_type = (
#                         "å…§éƒ¨å¼•ç”¨" if full_law == current_law
#                         else "ä¸»æ³•æ´å¼•" if current_law in LAW_RELATIONS and full_law == LAW_RELATIONS[current_law]
#                         else "è·¨æ³•æº"
#                     )
#                     results.append({
#                         "law_name": full_law,
#                         "article": article,
#                         "paragraph": p,
#                         "subparagraph": s,
#                         "type": ref_type
#                     })

#     implicit_pattern = re.compile(
#         r"(?<!æ³•)(?<![\u4e00-\u9fa5])(?<!ä¾)(?<!æ ¹æ“š)(?<!æŒ‰)(?<!åƒç…§)"
#         r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
#         r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*))?"
#         r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*)?"
#     )

#     for match in implicit_pattern.finditer(content):
#         article, item_block, subitem_block = match.groups()

#         paragraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …", item_block or "") or [None]
#         subparagraphs = re.findall(
#             r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)",
#             subitem_block or ""
#         )
#         subparagraphs = [s.replace("æˆ–", "") for s in subparagraphs] if subparagraphs else [None]

#         for p in paragraphs:
#             for s in subparagraphs:
#                 results.append({
#                     "law_name": current_law,
#                     "article": article,
#                     "paragraph": p,
#                     "subparagraph": s,
#                     "type": "å…§éƒ¨å¼•ç”¨"
#                 })

#     return results


# # def extract_references(content, current_law):
# #     if not content:
# #         return []

# #     results = []

# #     if current_law in LAW_RELATIONS:
# #         alias_map = {
# #             "æœ¬æ³•": LAW_RELATIONS[current_law],
# #             "æœ¬ç´°å‰‡": current_law
# #         }
# #     else:
# #         alias_map = {
# #             "æœ¬æ³•": current_law,
# #             "æœ¬ç´°å‰‡": current_law
# #         }

# #     explicit_block_pattern = re.compile(
# #         r"(?:ä¾|æ ¹æ“š|æŒ‰|åƒç…§)[\sã€Œã€]*"
# #         r"(æœ¬æ³•|æœ¬ç´°å‰‡|[\u4e00-\u9fa5]{2,6}æ³•)"
# #         r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
# #         r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*))?"
# #         r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*)?"
# #     )

# #     extra_reference_pattern = re.compile(
# #         r"(æœ¬æ³•|æœ¬ç´°å‰‡|[\u4e00-\u9fa5]{2,}æ³•)"
# #         r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
# #         r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)?"
# #         r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)?"
# #         r"(æ‰€ç¨±|æ‰€å®š|æ‰€è¬‚)"
# #     )

# #     for pattern in [explicit_block_pattern, extra_reference_pattern]:
# #         for match in pattern.finditer(content):
# #             law_name, article, item_block, subitem_block = match.groups()[:4]
# #             full_law = alias_map.get(law_name, law_name)

# #             paragraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …", item_block or "") or [None]
# #             subparagraphs = re.findall(
# #                 r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)",
# #                 subitem_block or ""
# #             )
# #             subparagraphs = [s.replace("æˆ–", "") for s in subparagraphs] if subparagraphs else [None]

# #             for p in paragraphs:
# #                 for s in subparagraphs:
# #                     results.append({
# #                         "law_name": full_law,
# #                         "article": article,
# #                         "paragraph": p,
# #                         "subparagraph": s
# #                     })

# #     implicit_pattern = re.compile(
# #         r"(?<!æ³•)(?<![\u4e00-\u9fa5])(?<!ä¾)(?<!æ ¹æ“š)(?<!æŒ‰)(?<!åƒç…§)"
# #         r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¢(?:ä¹‹[\d]+)?)"
# #         r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …)*))?"
# #         r"((?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾(?:ã€ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)*)?"
# #     )

# #     for match in implicit_pattern.finditer(content):
# #         article, item_block, subitem_block = match.groups()

# #         paragraphs = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+é …", item_block or "") or [None]
# #         subparagraphs = re.findall(
# #             r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾|æˆ–ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+æ¬¾)",
# #             subitem_block or ""
# #         )
# #         subparagraphs = [s.replace("æˆ–", "") for s in subparagraphs] if subparagraphs else [None]

# #         for p in paragraphs:
# #             for s in subparagraphs:
# #                 results.append({
# #                     "law_name": current_law,
# #                     "article": article,
# #                     "paragraph": p,
# #                     "subparagraph": s
# #                 })

# #     return results

# """ è™•ç† immigration-law.json æ ¼å¼ """
# def handle_law(json_data, index_name):
#     # åˆ¤æ–·æ˜¯å¦ç‚ºç« ç¯€åˆ¶
#     is_chapter_based = "ç« ç¯€" in json_data

#     # æ ¹æ“šçµæ§‹å–å¾—æ¢æ–‡æ¸…å–®
#     if is_chapter_based:
#         documents = json_data.get("ç« ç¯€", [])
#     else:
#         documents = [{"ç« å": None, "æ¢æ–‡": json_data.get("æ¢æ–‡", [])}]

#     if not documents or all(not chapter.get("æ¢æ–‡") for chapter in documents):
#         print(f"âš ï¸ æª”æ¡ˆç¼ºå°‘æ¢æ–‡è³‡æ–™")
#         return 0, 0

#     # å»ºç«‹ indexï¼ˆè‹¥å°šæœªå­˜åœ¨ï¼‰
#     if not es.indices.exists(index=index_name):
#         es.indices.create(index=index_name, body={
#             "mappings": {
#                 "properties": {
#                     "name": {"type": "keyword"},
#                     "date": {"type": "keyword"},
#                     "chapter_title": {"type": "keyword"},
#                     "number": {"type": "keyword"},
#                     "reference_laws": {
#                         "type": "nested",
#                         "properties": {
#                             "law_name": {"type": "keyword"},
#                             "article": {"type": "keyword"},
#                             "paragraph": {"type": "keyword"},
#                             "subparagraph": {"type": "keyword"}
#                         }
#                     },
#                     "content": {"type": "text"},
#                     "embedding": {"type": "dense_vector", "dims": 512}
#                 }
#             }
#         })
#         print(f"âœ… å»ºç«‹ indexï¼š{index_name}")
#     else:
#         print(f"â„¹ï¸ ç´¢å¼•å·²å­˜åœ¨ï¼š{index_name}")

#     success, failed = 0, 0
#     for chapter in documents:
#         chapter_title = chapter.get("ç« å", "") or ""
#         for clause in chapter.get("æ¢æ–‡", []):
#             clause_id = clause.get("æ¢è™Ÿ")
#             content = clause.get("å…§å®¹", "")

#             if not clause_id or not content:
#                 failed += 1
#                 continue

#             embedding = model.encode(content).tolist()
#             doc_id = f"{chapter_title}_{clause_id}".replace(" ", "")

#             # åƒç…§æ³•æº
#             reference_laws = extract_references(content, json_data.get("æ³•è¦åç¨±", ""))

#             # è‹¥å·²å­˜åœ¨å°±è·³éè™•ç†
#             if es.exists(index=index_name, id=doc_id):
#                 continue

#             try:
#                 es.index(index=index_name, id=doc_id, body={
#                     "name": json_data.get("æ³•è¦åç¨±", ""),
#                     "date": json_data.get("ä¿®æ­£æ—¥æœŸ", ""),
#                     "chapter_title": chapter_title,
#                     "number": clause_id,
#                     "reference_laws": reference_laws, 
#                     "content": content,
#                     "embedding": embedding
#                 })
#                 success += 1
#             except Exception as e:
#                 print(f"âŒ å¯«å…¥å¤±æ•—ï¼š{clause_id} - {e}")
#                 failed += 1

#     return success, failed


# """ åœ¨é€™è£¡å¢åŠ å…¶ä»–è™•ç†æ ¼å¼ """


# """ è³‡æ–™ä¾†æºè·¯å¾‘ """
# DATA_DIR = "data"
# files = [f for f in os.listdir(DATA_DIR) if f.endswith(".json")]

# """ æ ¹æ“šæª”åé¸æ“‡è™•ç†é‚è¼¯ """
# for filename in files:
#     path = os.path.join(DATA_DIR, filename)
#     try:
#         with open(path, "r", encoding="utf-8") as f:
#             json_data = json.load(f)
#     except Exception as e:
#         print(f"ğŸ”´ ç„¡æ³•è®€å– {filename}ï¼š{e}")
#         continue

#     # è‡ªå‹•ç”¨æª”åè½‰ index å
#     base = os.path.splitext(filename)[0]  # e.g. "labor-law"
#     index_name = f"ai_{base}_index"

#     print(f"\nğŸ“„ è™•ç†æª”æ¡ˆï¼š{filename} â†’ Index: {index_name}")

#     # æ ¹æ“šæª”æ¡ˆåç¨±æ±ºå®šæ ¼å¼è™•ç†æ–¹å¼
#     # if "labor-law" in base:
#     #     success, failed = handle_labor_law(json_data, index_name)
#     if any(keyword in base for keyword in ["immigration-law", "immigration-regulations", "nationality-law"]):
#         success, failed = handle_law(json_data, index_name)
#     else:
#         print(f"âš ï¸ å°šæœªæ”¯æ´æ­¤æª”æ¡ˆæ ¼å¼ï¼š{filename}")
#         continue

#     print(f"âœ… å¯«å…¥æˆåŠŸï¼š{success} ç­†ï¼Œâš ï¸ å¤±æ•—ï¼š{failed} ç­†")

# print("\nğŸ“Œ æ‰€æœ‰æª”æ¡ˆè™•ç†å®Œæˆï¼")





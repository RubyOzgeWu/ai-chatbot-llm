from elasticsearch import Elasticsearch
from sentence_transformers import SentenceTransformer
import os
import re
from dotenv import load_dotenv

load_dotenv()

RUNNING_IN_DOCKER = os.path.exists('/.dockerenv')
ELASTICSEARCH_HOST = (
    os.getenv("ELASTICSEARCH_HOSTS", "http://elasticsearch:9200")
    if RUNNING_IN_DOCKER
    else os.getenv("ELASTICSEARCH_HOSTS_LOCAL", "http://localhost:9200")
)

es = Elasticsearch(ELASTICSEARCH_HOST)
embedding_model = SentenceTransformer("sentence-transformers/distiluse-base-multilingual-cased-v2")

CHINESE_NUMERAL_MAP = {
    "Èõ∂": 0, "‰∏Ä": 1, "‰∫å": 2, "‰∏â": 3, "Âõõ": 4,
    "‰∫î": 5, "ÂÖ≠": 6, "‰∏É": 7, "ÂÖ´": 8, "‰πù": 9,
    "ÂçÅ": 10, "Áôæ": 100, "ÂçÉ": 1000
}

def chinese_numeral_to_int(text):
    if text.isdigit():
        return int(text)

    result = 0
    tmp = 0
    units = {"ÂçÅ": 10, "Áôæ": 100, "ÂçÉ": 1000}
    i = 0
    while i < len(text):
        ch = text[i]
        if ch in units:
            unit = units[ch]
            if tmp == 0:
                tmp = 1
            result += tmp * unit
            tmp = 0
        else:
            tmp = CHINESE_NUMERAL_MAP.get(ch, 0)
        i += 1
    result += tmp
    return result

def normalize_article(article_text):
    m = re.match(r"Á¨¨([‰∏Ä‰∫å‰∏âÂõõ‰∫îÂÖ≠‰∏ÉÂÖ´‰πùÂçÅÁôæÈõ∂\d]+)Ê¢ù", article_text)
    if not m:
        return article_text

    raw = m.group(1)

    if raw.isdigit():  # ÊòØÈòøÊãâ‰ºØÊï∏Â≠óÔºåÁõ¥Êé•Áî®
        return f"Á¨¨{raw}Ê¢ù"

    # ÊòØ‰∏≠ÊñáÊï∏Â≠óÊâçËΩâÊèõ
    arabic = chinese_numeral_to_int(raw)
    return f"Á¨¨{arabic}Ê¢ù"

def retrieve_similar_docs(query, index, top_k=3, use_reference_expansion=False):
    query_embedding = embedding_model.encode(query).tolist()
    search_body = {
        "size": top_k,
        "query": {
            "script_score": {
                "query": {"match_all": {}},
                "script": {
                    "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                    "params": {"query_vector": query_embedding}
                }
            }
        }
    }

    try:
        response = es.search(index=index, body=search_body)
        docs = [hit["_source"] for hit in response["hits"]["hits"]]
    except Exception as e:
        print(f"‚ùå Elasticsearch Êü•Ë©¢Â§±Êïó: {e}")
        return []

    if use_reference_expansion:
        extra_docs = []
        for doc in docs:
            for ref in doc.get("reference_laws", []):
                try:
                    raw_article = ref["article"]
                    normalized_article = normalize_article(raw_article)
                    law_name = ref["law_name"]

                    print(f"\nüìå ÂºïÁî®Ê¢ùÊñáÊì¥ÂÖÖ‰∏≠Ôºö")
                    print(f"üîπ law_name: {law_name}")
                    print(f"üîπ article (raw): {raw_article} ‚Üí (normalized): {normalized_article}")

                    ref_query = {
                        "query": {
                            "bool": {
                                "must": [
                                    {"match": {"number": normalized_article}},
                                    {"match": {"name": law_name}},
                                    # {"term": {"name.keyword": law_name}}
                                ]
                            }
                        }
                    }

                    print(f"üß™ Êü•Ë©¢Ê¢ù‰ª∂: {ref_query}")
                    ref_index = index  # Áî®Áõ∏ÂêåÁöÑ index Êü•
                    ref_res = es.search(index=ref_index, body=ref_query)
                    hits = ref_res["hits"]["hits"]
                    print(f"üîç Êì¥ÂÖÖÊü•Ë©¢ÂëΩ‰∏≠Ôºö{len(hits)} Á≠Ü")

                    for hit in hits:
                        print(f"‚úî ÂëΩ‰∏≠Ê¢ùÊñáÔºö{hit['_source'].get('number')} - {hit['_source'].get('content', '')[:30]}...")
                        extra_docs.append(hit["_source"])

                except Exception as e:
                    print(f"‚ö† Êì¥ÂÖÖÊ¢ùÊñáÊü•Ë©¢Â§±Êïó: {e}")

        docs.extend(extra_docs)


    return docs

# -------- Ê∏¨Ë©¶Ë≥áÊñô -------- #
ground_truth = [
    {
        "query": "Êú™ÂèñÂæóÂ§ñÂúãÂúãÁ±çÊòØÂê¶ÂèØ‰ª•Êí§Èä∑ÂúãÁ±çÂñ™Â§±Ôºü",
        "relevant_articles": ["Á¨¨11Ê¢ù"]
    }
]

def evaluate_precision_at_k(testset, top_k=3, index="", label="", use_reference_expansion=False):
    hit_count = 0

    print(f"\nüöÄ Ê∏¨Ë©¶Á¥¢ÂºïÊ®°ÂºèÔºö{label}Ôºàindex: {index}, Â±ïÈñãÂºïÁî®: {use_reference_expansion}Ôºâ\n")
    for item in testset:
        query = item["query"]
        relevant = item["relevant_articles"]
        results = retrieve_similar_docs(query, index=index, top_k=top_k, use_reference_expansion=use_reference_expansion)
        retrieved_ids = [r.get("number", "") for r in results]

        match = any(rel == rid for rel in relevant for rid in retrieved_ids)
        if match:
            hit_count += 1
        else:
            print(f"‚ùå Êú™ÂëΩ‰∏≠Ôºö{query}")
            print(f"‚Üí ÊúüÊúõ: {relevant}")
            print(f"‚Üí ÂØ¶Èöõ: {retrieved_ids}")
            print("-" * 40)

    precision = hit_count / len(testset)
    print(f"\nüìä Precision@{top_k}Ôºà{label}Ôºâ: {precision:.2f}")

# -------- CLI ‰∏ªÂÖ•Âè£ -------- #
if __name__ == "__main__":
    index = "ai_nationality-law_index"

    evaluate_precision_at_k(
        ground_truth,
        top_k=3,
        index=index,
        label="‚ùå ÁÑ°ÂºïÁî®Ê¢ùÊñáÊì¥ÂÖÖ",
        use_reference_expansion=False
    )

    evaluate_precision_at_k(
        ground_truth,
        top_k=3,
        index=index,
        label="‚úÖ ÊúâÂºïÁî®Ê¢ùÊñáÊì¥ÂÖÖ",
        use_reference_expansion=True
    )

""" Postman å–®ç´” fastAPI """
from elasticsearch import Elasticsearch
from sentence_transformers import SentenceTransformer
# import google.generativeai as genai
from openai import OpenAI
from dotenv import load_dotenv
import os

""" è¨­ç½®ç’°å¢ƒ """
# è¼‰å…¥ .env æª”æ¡ˆ
load_dotenv()

# è®€å–ç’°å¢ƒè®Šæ•¸
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("âŒ ç¼ºå°‘ OPENAI_API_KEY ç¯å¢ƒå˜é‡ï¼Œè¯·ç¡®è®¤ .env è®¾ç½®ï¼")
# GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
# if not GOOGLE_API_KEY:
#     raise ValueError("âŒ ç¼ºå°‘ GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸ï¼Œè«‹ç¢ºèª .env è¨­å®šï¼")

# åˆ¤æ–·æ˜¯å¦åœ¨ Docker å®¹å™¨å…§é‹è¡Œ
RUNNING_IN_DOCKER = os.path.exists('/.dockerenv')

if RUNNING_IN_DOCKER:
    ELASTICSEARCH_HOST = os.getenv("ELASTICSEARCH_HOSTS", "http://elasticsearch:9200")
else:
    ELASTICSEARCH_HOST = os.getenv("ELASTICSEARCH_HOSTS_LOCAL", "http://localhost:9200")


# ç¢ºèªç’°å¢ƒè®Šæ•¸æ˜¯å¦æ­£ç¢ºè¼‰å…¥
# print(f"ğŸ” GOOGLE_API_KEY: {GOOGLE_API_KEY}")
# print(f"ğŸ” ELASTICSEARCH_HOST: {ELASTICSEARCH_HOST}")


""" é€£æ¥ Elasticsearch """
es = Elasticsearch(ELASTICSEARCH_HOST)

""" å‘é‡åŒ–æ¨¡å‹ """
embedding_model = SentenceTransformer("sentence-transformers/distiluse-base-multilingual-cased-v2")


# æ¸¬è©¦é€£æ¥
try:
    es.info()
    print("âœ… Elasticsearch é€£æ¥æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ ç„¡æ³•é€£æ¥åˆ° Elasticsearch: {e}")

""" å°å…¥ LLM æ¨¡å‹ """
client = OpenAI(api_key=OPENAI_API_KEY)
model_name = "gpt-3.5-turbo"
# genai.configure(api_key=GOOGLE_API_KEY)
# model_name = "gemini-1.5-pro"  
# llm = genai.GenerativeModel(model_name)


""" RAG å‘é‡æª¢ç´¢æ–¹æ³• """
def retrieve_similar_docs(query, index="ai_labor-law_index", top_k=3):
    query_embedding = embedding_model.encode(query).tolist()

    search_body = {
        "size": top_k,
        "query": {
            "script_score": {
                "query": {"match_all": {}},
                "script": {
                    "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                    "params": {"query_vector": query_embedding}
                }
            }
        }
    }

    try:
        if not es.indices.exists(index=index):
            print(f"âŒ éŒ¯èª¤ï¼šç´¢å¼• '{index}' ä¸å­˜åœ¨ï¼è«‹å…ˆå»ºç«‹ç´¢å¼•ä¸¦è¼‰å…¥æ•¸æ“šï¼")
            return []

        response = es.search(index=index, body=search_body)

        return [
            {
                "æ³•è¦åç¨±": hit["_source"].get("name", ""),
                "ä¿®æ­£æ—¥æœŸ": hit["_source"].get("date", ""),
                "ç« ç¯€æ¨™é¡Œ": hit["_source"].get("chapter_title", ""),
                "æ¢è™Ÿ": hit["_source"].get("number", ""),
                "å…§å®¹": hit["_source"].get("content", ""),
            }
            for hit in response["hits"]["hits"]
        ]

    except Exception as e:
        print(f"âŒ Elasticsearch æŸ¥è©¢å¤±æ•—: {e}")
        return []

""" RAG  æª¢ç´¢ + ç”Ÿæˆå›æ‡‰ """   
def rag_fastapi(user_query):
    # RAG æª¢ç´¢
    docs = retrieve_similar_docs(user_query)
    print(docs)

    if not docs:
        return  {
            "answer": "æ‰¾ä¸åˆ°ç›¸é—œæ¢æ–‡ã€‚",
            "references": []
        }

    # RAG æœå°‹ 
    context = "\n\n".join([
        f"{doc['æ³•è¦åç¨±']}ï¼ˆ{doc['ä¿®æ­£æ—¥æœŸ']}ï¼‰\n{doc['ç« ç¯€æ¨™é¡Œ']} {doc['æ¢è™Ÿ']}\n{doc['å…§å®¹']}"
        for doc in docs
    ])

    prompt = f"æ ¹æ“šä»¥ä¸‹è³‡è¨Šå›ç­”å•é¡Œ:\n{context}\n\nå•é¡Œ: {user_query}"

    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )

        return {
            "answer": response.choices[0].message.content.strip()
        }

    except Exception as e:
        print(f"âŒ OpenAI å›å‚³éŒ¯èª¤: {e}")
        return {
            "answer": "ç”¢ç”Ÿå›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚",
            "references": []
        }
    # response = openai.ChatCompletion.create(
    #     model=model_name,
    #     messages=[
    #         {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚"},
    #         {"role": "user", "content": prompt}
    #     ]
    # )

    # return {
    #     "answer": response["choices"][0]["message"]["content"].strip()
    # }

    # response = llm.generate_content(prompt)

    # return {
    #   "answer": response.text
    # }

